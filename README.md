# AI MLOps Project

[![Python](https://img.shields.io/badge/python-3.8%2B-blue)](https://www.python.org/)
[![Spark](https://img.shields.io/badge/Spark-3.5.5-orange)](https://spark.apache.org/)
[![Docker](https://img.shields.io/badge/docker-20.10-blue)](https://www.docker.com/)
[![MLflow](https://img.shields.io/badge/MLflow-2.6.2-green)](https://mlflow.org/)
[![LLM](https://img.shields.io/badge/GenerativeAI-OpenAI-blueviolet)](https://openai.com/)
[![Build](https://github.com/impesud/ai-mlops-project/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/impesud/ai-mlops-project/actions/workflows/ci-cd.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/impesud/ai-mlops-project/blob/main/LICENSE)
[![Last Commit](https://img.shields.io/github/last-commit/impesud/ai-mlops-project)](https://github.com/impesud/ai-mlops-project/commits/main)
[![Platform](https://img.shields.io/badge/platform-Ubuntu-blue)]()

ğŸš€ **AI MLOps Project** â€“ A production-grade MLOps pipeline combining Big Data processing, scalable training workflows, and LLM-powered analytics.

This end-to-end solution features:  
- Data ingestion from S3 using Apache Spark  
- Model training with full MLflow tracking (parameters, metrics, models, artifacts)  
- Automated data reporting using OpenAI's Generative AI  
- Testing via `pytest`  
- CI/CD workflows powered by GitHub Actions  
- Dockerized deployment pipeline

âœ… Fully compatible with Ubuntu + GitHub Actions  
âœ… Cloud extensible (AWS, Azure, GCP)  
âœ… Docker-ready with automated image build and deployment  
âœ… Modular design for fast customization and reuse

---

## ğŸ“‹ Table of Contents

1. [Model and Training Pipeline](#-model-and-training-pipeline)  
2. [Requirements](#-requirements)  
3. [Ubuntu Setup](#-ubuntu-setup)  
4. [Components](#-components)  
5. [Usage Example](#-usage-example)  
6. [MLflow UI](#-mlflow-ui)  
7. [MLOps and Tracking Server](#-mlops-and-tracking-server)  
8. [Testing](#-testing)  
9. [CI/CD Pipeline](#-cicd-pipeline)  
10. [Next Steps](#-next-steps)  
11. [License](#-license)

---

## ğŸ” Model and Training Pipeline

Starting from a synthetic CSV file, the project performs batch ingestion with Apache Spark, followed by feature engineering in Pandas. The preprocessed data is used to train a Random Forest classifier on a binary classification task: purchase vs. non-purchase.

The model predicts, for each event with its features (including value and timestamp), whether it's a purchase (1) or another action (0, e.g., click/view/signup). Results are logged with MLflow, and reports are generated via OpenAI.

---

## ğŸ”§ Requirements

* **Python** 3.8+ (3.10 or 3.11 recommended)  
* **Java** 8+ (for Apache Spark)  
* **Docker**  
* **MLflow** CLI (`pip install mlflow`)  
* **AWS CLI** / GCP SDK / Azure CLI  
* **Git**  
* **OpenAI API key** (for generative AI)

---

## âš™ï¸ Ubuntu Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/impesud/ai-mlops-project.git
   cd ai-mlops-project
   ```

2. **Create and activate virtualenv**
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install --upgrade pip setuptools
   pip install -r requirements.txt
   ```

3. **Configure AWS profile**  
   Edit `~/.aws/config`:
   ```ini
   [<AWS_user>]
   region = eu-central-1
   ```
   Export environment variable:
   ```bash
   export AWS_PROFILE=<AWS_user>
   ```

4. **Set OpenAI key**  
   Add in `~/.bashrc` or `~/.zshrc`:
   ```bash
   export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxx"
   ```
   Then run:
   ```bash
   source ~/.bashrc
   ```

5. **Update `config.yaml`**  
   Edit `data_ingestion/config.yaml`:
   ```yaml
   format: csv
   path: s3a://my-mlops-raw-data/
   output_path: s3a://my-mlops-processed-data/
   local_output_path: data/processed
   aws:
     region: eu-central-1
   test_size: 0.2
   random_seed: 42
   model_params:
     n_estimators: 100
     max_depth: 5
   generative_ai:
     enabled: true
     prompt: "Data analysis"
     output_path: "report.txt"
   ```

---

## ğŸ§© Components

* `data_ingestion/`: Spark batch/streaming, data cleaning, write to S3/local  
* `data_processing/`: notebooks + transformations  
* `models/`: training (`train.py`), SMOTE, feature engineering, MLflow tracking  
* `scripts/`: orchestrator `pipeline.py` (ingestion, training, AI, logging)  
* `mlops/`: `entrypoint.sh` for MLflow Tracking Server  
* `generative_ai/`: `generate.py` for LLM reports (OpenAI)  
* `.github/workflows/`: CI/CD and automated testing

---

## ğŸ¯ Usage Example

```bash
# Activate virtualenv
source venv/bin/activate

# Full pipeline
python scripts/pipeline.py

# Separate steps
bash scripts/run_ingest.sh
bash scripts/run_train.sh

# Generate AI report
python generative_ai/generate.py --prompt "Data analysis" --output report.txt
```

---

## ğŸ“Š MLflow UI

Launch MLflow UI:
```bash
mlflow ui --backend-store-uri ./mlruns --port 5000
```

Access: [http://localhost:5000](http://localhost:5000)  
View:
1. Parameters and metrics  
2. Model and signature  
3. `report.txt` artifact generated by LLM

---

## âš™ï¸ MLOps and Tracking Server

Start MLflow Tracking Server:
```bash
bash mlops/entrypoint.sh
```
Access:
```bash
http://localhost:5000
```
Can be used in Docker containers or local networks.

---

## ğŸ§ª Testing

The `test/` folder includes automated tests with `pytest`.

| File             | Validates                                   |
|------------------|---------------------------------------------|
| `test_ingest.py` | Generation of Parquet files                |
| `test_train.py`  | Model logging and artifact in MLflow       |

Run all tests:
```bash
pytest test/
```

---

### ğŸ” **CI/CD Pipeline**

The GitHub Actions pipeline executes the following steps:

- âœ… Set up Python environment and install all dependencies  
- ğŸ”„ Run full data ingestion and model training workflow  
- ğŸ“Š Track parameters, metrics, models, and artifacts using MLflow  
- ğŸ§ª Execute unit tests with `pytest`  
- â˜ï¸ Upload the trained model artifact to GitHub Actions  
- ğŸ³ Build and push the Docker image to Docker Hub  
- ğŸ“¦ Deploy and test the inference endpoint using `mlflow models serve`

---

### ğŸš€ **Next Steps**

- ğŸ§  Advanced feature engineering (hour, weekday, user-level stats)  
- ğŸ¯ Hyperparameter tuning via Optuna or GridSearchCV  
- âš™ï¸ Containerization + Helm charts for Docker/Kubernetes deployment  
- âœ¨ Dynamic prompt generation + structured artifact logging  
- ğŸ” Enhanced CI/CD with SonarQube and multi-environment pipelines  
- ğŸ“¡ Model serving via `mlflow models serve` or FastAPI + REST API

---

## ğŸ“œ License

MIT Â© 2025 Erick Jara - Impesud

âœï¸ Attribution:  
If you use this project, please mention  
"Based on the AI MLOps Project by Erick Jara - Impesud".



